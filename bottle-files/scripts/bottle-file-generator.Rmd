---
title: "Hakai Institute Bottle File Data Quality Report"
author: "Brett Johnson"
date: '`r date()`'
output:
 html_document:
   theme: cosmo
   code_folding: hide
   toc: true
   toc_float: true
   number_sections: false
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE, warning = FALSE, message = FALSE, error = FALSE)
library(tidyverse)
library(devtools)
library(lubridate)
library(knitr)
library(here)
library(car)

# Read data (see data import script for data access code)
chl_all_cols <- read_csv(here("raw_data", "cha_all_cols.csv"), guess_max = 20000) 
nuts_all_cols <- read_csv(here("raw_data", "nuts_all_cols.csv"),
                          guess_max = 100000, 
                          col_types = cols(no2_no3_ugl = col_double(),
                                           tp = col_double())) 
poms_all_cols <- read_csv(here("raw_data", "poms_all_cols.csv"), guess_max = 20000) 
qu39_ctd <- read_csv(here("raw_data", "qu39_ctd.csv"), guess_max = 100000)

# Define common columns I want back from every discrete sample
common_columns <- c("event_pk", "date",  "collected", "hakai_id", "work_area", 
                    "site_id", "lat", "long","line_out_depth", "sampling_bout",
                    "pressure_transducer_depth")


#TODO: include flag columns for each variable
#TODO: Look out for cases when there is no start_dt
#TODO: break-out files by event_pk just before putting into erddap
#TODO: eventually add comments
#TODO: Confirm missingness with OSV?
#Note: xml has to be updated with new variable names each time
```

# Discrete Samples

The point of this script is to join discrete sample results taken from Niskn bottle with phsyical and biogeochemical parameters from paired CTD casts. 

Along the way I write various quality checks to summarize the quality of various samples.

## Chlorophyll a

```{r Chla QC, include = FALSE}
kable(chl_all_cols %>%  group_by(quality_level) %>% 
  summarize(n = n()), caption = "Table. Quality levels of chl a data" )

chl_all_cols %>% 
  group_by(work_area, quality_level) %>% 
  summarize(n_samples = n()) %>% 
  ggplot(aes(x = work_area, y = n_samples, fill = quality_level)) +
  geom_bar(stat = "identity") +
  labs(title = "Chla samples")

chl_all_cols %>% 
  group_by(work_area, chla_flag) %>% 
  summarize(n_samples = n()) %>% 
  ggplot(aes(x = work_area, y = n_samples, fill = chla_flag)) +
  geom_bar(stat = "identity")+
  labs(title = "Chla samples")
```

```{r Chl a}
chl <- chl_all_cols %>% 
  select(common_columns, filter_type, 
         chla_final, phaeo_final, chl_phaeo_quality_level = quality_level) %>% 
  filter(!filter_type %in% c('2 + 3', '20')) %>% 
  drop_na(filter_type, chla_final, phaeo_final) %>% 
  ungroup() %>% 
  filter(site_id == "QU39")

collapse_all_rows <- function(df) {
  return(coalesce(!!! as.list(df)))
}

chl_wide <- chl %>% 
  pivot_wider(names_from = c(filter_type), values_from = c(chla_final, phaeo_final)) %>% 
  group_by(event_pk, line_out_depth) %>% 
  summarise_all(collapse_all_rows) %>% 
  select(event_pk:chl_phaeo_quality_level, "chla_final_Bulk GF/F", "chla_final_GF/F") %>% 
  ungroup()

```

## Nutrients

```{r Nuts QC, include=FALSE}
kable(nuts_all_cols %>% group_by(quality_level) %>% 
  summarize(n = n()))

nuts_all_cols %>% 
  group_by(work_area, quality_level) %>% 
  summarize(n_samples = n()) %>% 
  ggplot(aes(x = work_area, y = n_samples, fill = quality_level)) +
  geom_bar(stat = "identity")+
  labs(title = "Nutrient samples")

```

```{r nuts replicates}

# The intent with this code chunk is to calculate the coeff. of variation for replicated samples. At some point we need to confirm when to throw out data based on too high of a CV.

nuts_replicates_qc <- nuts_all_cols %>% 
  select(common_columns, nh4_, no2_no3_um, no2_no3_ugl, no2_no3_units, tp, tdp,
         tn, tdn, srp, po4, sio2, po4pfilt, no3nfilt, po4punfl, no3nunfl,
         nh4nunfl, quality_level) %>% 
  group_by(event_pk, line_out_depth) %>% 
  summarise_at(c("nh4_", "no2_no3_um", "no2_no3_ugl",
           "tp", "tdp", "tn", "tdn", "srp", "po4", "sio2", "po4pfilt", "no3nfilt", "po4punfl",
           "no3nunfl", "nh4nunfl"), funs(mean(., na.rm = TRUE), sd(., na.rm = TRUE))) %>% 
  mutate(n_replicates = n(),
         nh4_cv         = (nh4__sd / nh4__mean) * 100,
         no2_no3_um_cv  = (no2_no3_um_sd / no2_no3_um_mean) * 100,
         no2_no3_ugl_cv = (no2_no3_ugl_sd / no2_no3_ugl_mean) * 100,
         tp_cv          = (tp_sd / tp_mean) * 100,
         tdp_cv         = (tdp_sd / tdp_mean) * 100,
         tn_cv          = (tn_sd / tn_mean) * 100,
         tdn_cv         = (tdn_sd / tdn_mean) * 100,
         srp_cv         = (srp_sd / srp_mean) * 100,
         po4_cv         = (po4_sd / po4_mean) * 100,
         sio2_cv        = (sio2_sd / sio2_mean) * 100,
         po4pfilt_cv    = (po4pfilt_sd / po4pfilt_mean) * 100,
         no3nfilt_cv    = (no3nfilt_sd / no3nfilt_mean) * 100,
         po4punfl_cv    = (po4punfl_sd / po4punfl_mean) * 100,
         no3nunfl_cv    = (no3nunfl_sd / no3nunfl_mean) * 100,
         nh4nunfl_cv    = (nh4nunfl_sd / nh4nunfl_mean) * 100
  ) %>% 
  filter(n_replicates > 1) %>% 
  ungroup() 

number_of_replicted_samping_events <- nrow(nuts_replicates_qc) 

# This lists the samples that have atleast one nutrient that has a CV > 10 (arbitrary)
high_cv <- nuts_replicates_qc %>% 
  select(
         nh4_cv,         
         no2_no3_um_cv,  
         no2_no3_ugl_cv, 
         tp_cv,          
         tdp_cv,         
         tn_cv,         
         tdn_cv,         
         srp_cv,         
         po4_cv,         
         sio2_cv,        
         po4pfilt_cv,    
         no3nfilt_cv,    
         po4punfl_cv,    
         no3nunfl_cv,    
         nh4nunfl_cv) %>% 
  filter_all(any_vars(. > 10))

nutty_nuts <- right_join(nuts_replicates_qc, high_cv) %>% 
  select(event_pk, line_out_depth, n_replicates: nh4nunfl_cv)

kable(nutty_nuts, caption = "Table. Nutrient sample replicates that have a coefficient of variation greater than .1")
```

```{r Nutrients}
nuts <- nuts_all_cols %>% 
  filter(site_id == "QU39") %>% 
    select(common_columns, nh4_, no2_no3_um, no2_no3_ugl,
           no2_no3_units, tp, tdp, tn, tdn, srp, po4, sio2, po4pfilt, no3nfilt, po4punfl,
           no3nunfl, nh4nunfl, quality_level)
  
mean_nuts <- nuts %>% 
  group_by(event_pk, line_out_depth, collected) %>% 
  summarise_at(c("nh4_", "no2_no3_um", "no2_no3_ugl",
           "tp", "tdp", "tn", "tdn", "srp", "po4", "sio2", "po4pfilt", "no3nfilt", "po4punfl",
           "no3nunfl", "nh4nunfl"), funs(mean(., na.rm = TRUE), sd(., na.rm = TRUE), n()))

```

## Particulate Organic Matter

```{r POMS QC, include=FALSE}
kable(poms_all_cols %>% group_by(quality_level) %>% 
  summarize(n = n()))

poms_all_cols %>% 
  group_by(work_area, quality_level) %>% 
  summarize(n_samples = n()) %>% 
  ggplot(aes(x = work_area, y = n_samples, fill = quality_level)) +
  geom_bar(stat = "identity") +
  labs(title = "POMS")
```


```{r POMS}
poms <- poms_all_cols %>% 
  filter(site_id == "QU39") %>% 
  select(common_columns, acidified, acidification_method, volume, part, screen_size, collected_poms = collected, preserved, analyzed, filter_portion, pre_weight_mg:row_flag, quality_level_poms = quality_level) %>% 
  ungroup()

```

# Join Discrete Samples

```{r join discrete samples}

discrete_samples <- full_join(chl_wide, nuts, by = c("event_pk", 
                                                     "line_out_depth", "site_id"),
                              suffix = c("_chl", "_nuts")) %>%
  full_join(poms, by = c("event_pk", "line_out_depth", "site_id")) %>% 
  ungroup() %>% 
  mutate(year = year(date),
         month = month(date),
         day = yday(date)) %>% 
  group_by(event_pk) %>% 
  mutate(min_bottle_collection_t = min(collected_chl, collected_nuts, collected_poms, na.rm = TRUE),
         max_bottle_collection_t = max(collected_chl, collected_nuts, collected_poms, na.rm = TRUE),
         time_diff_mins = (max_bottle_collection_t - min_bottle_collection_t) / 60, 
         discrete_sample_timediff_flag = ifelse(time_diff_mins > 60, "SVC", ""),
         median_discrete_collected = median(collected_chl, collected_nuts, collected_poms, 
                                            na.rm = TRUE)) 

write_csv(discrete_samples, here::here("processed_data", "discrete_samples.csv"))
```

## Discrete Sample QC

To assess whether there are any mistakes, or inconsistencies in how the data are
joined, I calculate the difference in the time collected for each sample and 
comapre it to the median time collected for all samples.

```{r Discrete samples QC}
#create minimal discrete samples data frame to qc easier
min_ds <- discrete_samples %>%  
  select(event_pk, collected_poms, collected_chl, collected_nuts, time_diff_mins,
         median_discrete_collected, discrete_sample_timediff_flag,
         min_bottle_collection_t, max_bottle_collection_t)

ds_qc_table <- min_ds %>% 
  filter(discrete_sample_timediff_flag != "") 

```

# CTD Data

```{r CTD}

qu39_ctd <- qu39_ctd %>% 
  mutate(month = month(date),
         depth = round(depth)) %>% # when I round it sometimes creates two rows for the same depth
  distinct_at(vars(date, depth), .keep_all = TRUE) # this removes the duplicated depths

```

# Join Discrete Samples and CTD Depths

```{r Bottle File Generation}
# switch here from tidyverse to data.table to use rolling join function.

library(data.table)
setDT(qu39_ctd)            ## convert to data.table by reference
setDT(discrete_samples)            ## same

qu39_ctd[, date := start_dt]  ## create a duplicate of 'date1'
setkey(qu39_ctd, start_dt)    ## set the column to perform the join on
setkey(discrete_samples, median_discrete_collected)    ## same as above

bottle_file <- qu39_ctd[discrete_samples, roll="nearest"] ## perform rolling join

write_csv(bottle_file, here::here("processed_data", "qu39_bottle_file.csv"), na = "")
```

```{r min bf qc}
#create minimal bottle file to QC
min_bf <- bottle_file %>% 
  select(event_pk, line_out_depth, date, end_dt, temperature, collected_chl, collected_nuts, collected_poms)

ctd_time_match_qc <- min_bf %>% 
  mutate(median_discrete = median(c(collected_nuts, collected_chl)),
         ctd_time_diff = abs(difftime(end_dt, median_discrete, units = "mins")),
         ctd_time_flag = ifelse(ctd_time_diff > 180, "SVC", "")) %>% 
  filter(ctd_time_flag != "")
  
  

kable(ctd_time_match_qc, caption = "Table. Events for which CTD collection time was more than 180 minutes")

```
